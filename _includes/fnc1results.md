# FNC-I Results
{: .row .col-lg-12 .text-center .section-heading}


<span class='row' markdown="1">
<div class='col-md-1'></div>

Fifty of the 80 participants made submissions for FNC-1 by the end of June 2nd using a wide array of techniques. The teams got access to the (unlabeled) test data and were scored automatically using the Codalab submission platform. The scoring system produces a raw score based on the differentially weighted [scoring metric](#fnc1-scoring). The relative score the raw score normalized by the maximum possible score on the test set.
{: .col-md-10 .whitepane .text-left .pane .row}

<span class='row' markdown="1">
<div class='col-md-2'></div>

|Rank|Team name   |Affliation  | Score  | Relative Score  |
|:---|:---|:---|---:|---:|
|1 |[SOLAT in the SWEN](https://github.com/Cisco-Talos/fnc-1)|Talos Intelligence|9556.50|82.02|
|2 |[Athene](https://github.com/hanselowski/athene_system)|TU Darmstadt|9550.75|81.97|
|3 |[UCL Machine Reading](https://github.com/uclmr/fakenewschallenge)|UCL|9521.50|81.72|
{: .col-md-8 .row .pane .whitepane}

<span class='row' markdown="1">
<div class='col-md-1'></div>

Congratulations to our top-3 teams! The top-3 teams will also get a cash prize of USD 1000, USD 600, and USD 400 respectively. In addition to the top-3 teams, we would also like to give a special shoutout to ranks 4 and 5 held by teams at UIUC and U. Arizona. For a complete leaderboard, visit the [competition's Codalab page](https://competitions.codalab.org/competitions/16843#results).
{: .col-md-10 .text-left .row .pane .whitepane}
